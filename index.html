<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Client-Side ONNX Video Detection</title>
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
    <style>
      body {
        font-family: sans-serif;
        padding: 20px;
      }
      .container {
        position: relative;
        display: inline-block;
      }
      video {
        display: block;
        max-width: 100%;
        border: 1px solid #ccc;
      }
      canvas {
        position: absolute;
        top: 0;
        left: 0;
        pointer-events: none;
        border: 2px solid red; /* Debug border to see canvas */
      }
      .controls {
        margin-bottom: 20px;
        background: #f0f0f0;
        padding: 15px;
        border-radius: 8px;
      }
    </style>
  </head>
  <body>
    <h1>ðŸš€ Client-Side ONNX Video Detector</h1>

    <div class="controls">
      <!--       <h3>1. Setup</h3>
      <label
        >Upload ONNX Model:
        <input type="file" id="modelInput" accept=".onnx" /></label
      ><br /><br />
      <label
        >Upload Video: <input type="file" id="videoInput" accept="video/*"
      /></label>
       -->
      <label style="display: none; margin-bottom: 10px">
        <input type="checkbox" id="useWorkerCheckbox" />
        Use Web Worker (uncheck for main thread comparison)
      </label>
      <p id="status">Status: Waiting for inputs...</p>
      <button
        id="startProcessing"
        style="
          display: none;
          padding: 10px 20px;
          font-size: 16px;
          cursor: pointer;
        "
      >
        Start Processing Video
      </button>
    </div>

    <div class="container">
      <video
        id="video"
        width="1140"
        height="640"
        controls
        muted
        style="display: none"
      ></video>
      <canvas
        id="canvas"
        width="1140"
        height="640"
        style="display: none"
      ></canvas>
    </div>

    <script>
      // --- CONFIGURATION ---
      const MODEL_INPUT_SIZE = 640;
      const CONFIDENCE_THRESHOLD = 0.5;

      // COCO Class Names (Standard YOLO)
      let labels = [
        "person",
        "bicycle",
        "car",
        "motorcycle",
        "airplane",
        "bus",
        "train",
        "truck",
        "boat",
        "traffic light",
        "fire hydrant",
        "stop sign",
        "parking meter",
        "bench",
        "bird",
        "cat",
        "dog",
        "horse",
        "sheep",
        "cow",
        "elephant",
        "bear",
        "zebra",
        "giraffe",
        "backpack",
        "umbrella",
        "handbag",
        "tie",
        "suitcase",
        "frisbee",
        "skis",
        "snowboard",
        "sports ball",
        "kite",
        "baseball bat",
        "baseball glove",
        "skateboard",
        "surfboard",
        "tennis racket",
        "bottle",
        "wine glass",
        "cup",
        "fork",
        "knife",
        "spoon",
        "bowl",
        "banana",
        "apple",
        "sandwich",
        "orange",
        "broccoli",
        "carrot",
        "hot dog",
        "pizza",
        "donut",
        "cake",
        "chair",
        "couch",
        "potted plant",
        "bed",
        "dining table",
        "toilet",
        "tv",
        "laptop",
        "mouse",
        "remote",
        "keyboard",
        "cell phone",
        "microwave",
        "oven",
        "toaster",
        "sink",
        "refrigerator",
        "book",
        "clock",
        "vase",
        "scissors",
        "teddy bear",
        "hair drier",
        "toothbrush",
      ];

      // labels = [
      //   "ball",
      //   "ball in basket",
      //   "player",
      //   "basket",
      //   "player_shooting",
      // ];

      // --- GLOBAL VARIABLES ---
      let session = null;
      let isProcessing = false;
      const video = document.getElementById("video");
      const canvas = document.getElementById("canvas");
      const ctx = canvas.getContext("2d");
      const status = document.getElementById("status");

      // Performance comparison flag
      let useWebWorker = document.getElementById("useWorkerCheckbox").checked;

      // Web Worker & Job Queue
      let worker = null;
      let jobQueue = [];
      let processingJobs = new Map();
      let frameIdCounter = 0;
      let maxConcurrentJobs = 3;
      let maxQueueSize = 10;
      let lastDisplayedFrameId = -1;
      let isWorkerReady = false;

      // Video preprocessing state
      let isPreprocessing = false;
      let preprocessingComplete = false;
      let frameAnnotations = new Map(); // Store all frame annotations
      let totalFramesToProcess = 0;
      let framesProcessed = 0;
      let videoFPS = 30; // Will be calculated from video
      let processingStartTime = 0;
      let processingTimerInterval = null;

      //   // --- 1. LOAD MODEL ---
      //   document
      //     .getElementById("modelInput")
      //     .addEventListener("change", async (e) => {
      //       const file = e.target.files[0];
      //       if (!file) return;

      //       status.textContent =
      //         "Status: Loading model... (this may take a moment)";

      //       try {
      //         const buffer = await file.arrayBuffer();
      //         session = await ort.InferenceSession.create(buffer, {
      //           executionProviders: ["webgpu"], // ["webgpu", "webgl", "wasm"],
      //         });
      //         status.textContent = "Status: âœ… Model Loaded! Now upload a video.";
      //       } catch (err) {
      //         status.textContent = `Status: âŒ Error loading model: ${err.message}`;
      //         console.error(err);
      //       }
      //     });

      //   // --- 2. LOAD VIDEO ---
      //   document.getElementById("videoInput").addEventListener("change", (e) => {
      //     const file = e.target.files[0];
      //     if (file) {
      //       const url = URL.createObjectURL(file);
      //       video.src = url;
      //       status.textContent =
      //         "Status: Video ready. Play the video to start detection.";
      //     }
      //   });

      const MODEL_PATH = "./yolo11n-detect.onnx";
      // const MODEL_PATH = "./best---dynamic.onnx";
      // const VIDEO_PATH = "./basket-sp.mp4";
      const VIDEO_PATH = "./input10s.mp4";

      // Listen to checkbox changes
      document.addEventListener("DOMContentLoaded", () => {
        const checkbox = document.getElementById("useWorkerCheckbox");
        useWebWorker = checkbox.checked;
        checkbox.addEventListener("change", (e) => {
          alert("Please reload the page for mode change to take effect.");
        });
      });

      // --- 1. AUTO-INIT ON LOAD ---
      window.onload = async () => {
        try {
          const checkbox = document.getElementById("useWorkerCheckbox");
          useWebWorker = checkbox.checked;

          if (useWebWorker) {
            status.textContent = "Status: Initializing Web Worker...";
            worker = new Worker("./detection-worker.js");

            // Setup worker message handler
            worker.onmessage = handleWorkerMessage;
            worker.onerror = (error) => {
              status.textContent = `Worker Error: ${error.message}`;
              console.error("Worker error:", error);
            };

            // B. Initialize model in worker
            worker.postMessage({
              type: "INIT",
              data: { modelPath: MODEL_PATH },
            });
          } else {
            // Main Thread Mode
            status.textContent = "Status: Loading model on main thread...";
            const modelResponse = await fetch(MODEL_PATH);
            const modelBuffer = await modelResponse.arrayBuffer();

            session = await ort.InferenceSession.create(modelBuffer, {
              executionProviders: ["webgl", "wasm"],
              graphOptimizationLevel: "all",
            });

            console.log("[Main Thread] Model loaded");
          }

          // C. Set Video Source
          video.src = VIDEO_PATH;

          // Wait for video metadata
          video.onloadedmetadata = () => {
            canvas.width = video.clientWidth;
            canvas.height = video.clientHeight;

            // Calculate total frames and FPS
            videoFPS = 30; // Default, actual FPS detection is complex in browser
            totalFramesToProcess = Math.ceil(video.duration * videoFPS);

            if (useWebWorker && isWorkerReady) {
              status.textContent = `Status: Ready! Click "Start Processing" to analyze video (${totalFramesToProcess} frames)`;
              document.getElementById("startProcessing").style.display =
                "inline-block";
            } else if (!useWebWorker) {
              status.textContent = `Status: Ready! Click "Start Processing" to analyze video (${totalFramesToProcess} frames)`;
              document.getElementById("startProcessing").style.display =
                "inline-block";
            }
          };
        } catch (err) {
          status.textContent = `Status: âŒ Error: ${err.message}`;
          console.error(err);
        }
      };

      // Start processing button handler
      document
        .getElementById("startProcessing")
        .addEventListener("click", () => {
          if (!isPreprocessing && !preprocessingComplete) {
            preprocessAllFrames();
          }
        });

      // Handle messages from worker
      function handleWorkerMessage(e) {
        const { type, frameId, output, processingTime, error } = e.data;

        switch (type) {
          case "MODEL_LOADED":
            isWorkerReady = true;
            if (video.readyState >= 1) {
              // HAVE_METADATA
              status.textContent = `Status: Ready! Click "Start Processing" to analyze video (${totalFramesToProcess} frames)`;
              document.getElementById("startProcessing").style.display =
                "inline-block";
            } else {
              status.textContent = "Status: Ready! Waiting for video...";
            }
            console.log("Worker ready with model");
            break;

          case "FRAME_PROCESSED":
            // Store detections for this frame
            if (output) {
              frameAnnotations.set(frameId, {
                data: new Float32Array(output.data),
                dims: output.dims,
              });
              framesProcessed++;

              // Update progress
              if (isPreprocessing) {
                const progress = (
                  (framesProcessed / totalFramesToProcess) *
                  100
                ).toFixed(1);
                const elapsedSeconds = (
                  (Date.now() - processingStartTime) /
                  1000
                ).toFixed(1);
                status.textContent = `Processing video: ${framesProcessed}/${totalFramesToProcess} frames (${progress}%) - Elapsed: ${elapsedSeconds}s`;
              }
            }

            // Remove from processing queue
            processingJobs.delete(frameId);

            // Process next job or finish
            if (isPreprocessing) {
              processNextPreprocessJob();
            } else {
              processNextJob();
            }
            break;

          case "ERROR":
            console.error("Worker error:", error);
            if (frameId !== undefined) {
              processingJobs.delete(frameId);
              if (isPreprocessing) {
                processNextPreprocessJob();
              } else {
                processNextJob();
              }
            }
            break;
        }
      }

      // Process next job from queue
      function processNextJob() {
        while (jobQueue.length > 0 && processingJobs.size < maxConcurrentJobs) {
          const job = jobQueue.shift();
          processingJobs.set(job.frameId, job);

          worker.postMessage({
            type: "PROCESS_FRAME",
            data: job,
          });
        }
      }

      // Process next job during preprocessing
      function processNextPreprocessJob() {
        while (jobQueue.length > 0 && processingJobs.size < maxConcurrentJobs) {
          const job = jobQueue.shift();
          processingJobs.set(job.frameId, job);

          if (useWebWorker) {
            worker.postMessage({
              type: "PROCESS_FRAME",
              data: job,
            });
          }
        }

        // Check if preprocessing is complete
        if (
          jobQueue.length === 0 &&
          processingJobs.size === 0 &&
          isPreprocessing
        ) {
          finishPreprocessing();
        }
      }

      // Preprocess all frames in the video
      async function preprocessAllFrames() {
        if (isPreprocessing || preprocessingComplete) return;

        isPreprocessing = true;
        framesProcessed = 0;
        frameAnnotations.clear();
        frameIdCounter = 0;
        processingStartTime = Date.now();
        document.getElementById("startProcessing").style.display = "none";
        status.textContent = "Starting video preprocessing... (0.0s)";

        // Start timer to update elapsed time
        processingTimerInterval = setInterval(() => {
          if (isPreprocessing) {
            const elapsedSeconds = (
              (Date.now() - processingStartTime) /
              1000
            ).toFixed(1);
            const progress = (
              (framesProcessed / totalFramesToProcess) *
              100
            ).toFixed(1);
            status.textContent = `Processing video: ${framesProcessed}/${totalFramesToProcess} frames (${progress}%) - Elapsed: ${elapsedSeconds}s`;
          }
        }, 100); // Update every 100ms

        console.log(
          `[Preprocess] Starting to process ${totalFramesToProcess} frames`,
        );

        // Seek through video and capture all frames
        video.currentTime = 0;
        const frameDuration = 1 / videoFPS;

        const captureFrame = async (frameNumber) => {
          return new Promise((resolve) => {
            const targetTime = frameNumber * frameDuration;

            video.onseeked = () => {
              // Capture frame
              const offScreenCanvas = document.createElement("canvas");
              offScreenCanvas.width = MODEL_INPUT_SIZE;
              offScreenCanvas.height = MODEL_INPUT_SIZE;
              const tempCtx = offScreenCanvas.getContext("2d");
              tempCtx.drawImage(
                video,
                0,
                0,
                MODEL_INPUT_SIZE,
                MODEL_INPUT_SIZE,
              );
              const imageData = tempCtx.getImageData(
                0,
                0,
                MODEL_INPUT_SIZE,
                MODEL_INPUT_SIZE,
              );

              resolve(imageData);
            };

            video.currentTime = Math.min(targetTime, video.duration - 0.001);
          });
        };

        // Process frames sequentially
        for (let frameNum = 0; frameNum < totalFramesToProcess; frameNum++) {
          const imageData = await captureFrame(frameNum);

          const job = {
            frameId: frameNum,
            timestamp: frameNum * frameDuration,
            imageData: {
              data: Array.from(imageData.data),
              width: imageData.width,
              height: imageData.height,
            },
          };

          if (useWebWorker) {
            jobQueue.push(job);
            processNextPreprocessJob();
          } else {
            // Main thread processing
            await processFrameMainThread(job);
          }

          // Wait a bit if queue is getting full
          while (
            jobQueue.length >= maxQueueSize ||
            processingJobs.size >= maxConcurrentJobs
          ) {
            await new Promise((resolve) => setTimeout(resolve, 10));
          }
        }

        // Wait for all processing to complete
        if (!useWebWorker) {
          finishPreprocessing();
        }
      }

      // Main thread frame processing
      async function processFrameMainThread(job) {
        const { frameId, timestamp, imageData } = job;

        try {
          const inputTensor = preprocessOnMainThread(
            new ImageData(
              new Uint8ClampedArray(imageData.data),
              imageData.width,
              imageData.height,
            ),
          );

          const feeds = {};
          feeds[session.inputNames[0]] = inputTensor;
          const results = await session.run(feeds);

          const outputName = session.outputNames[0];
          const outputTensor = results[outputName];

          frameAnnotations.set(frameId, {
            data: new Float32Array(outputTensor.data),
            dims: outputTensor.dims,
          });

          framesProcessed++;
          const progress = (
            (framesProcessed / totalFramesToProcess) *
            100
          ).toFixed(1);
          status.textContent = `Processing video: ${framesProcessed}/${totalFramesToProcess} frames (${progress}%)`;
        } catch (err) {
          console.error(`Error processing frame ${frameId}:`, err);
        }
      }

      // Finish preprocessing and enable video playback
      function finishPreprocessing() {
        isPreprocessing = false;
        preprocessingComplete = true;

        // Stop the timer
        if (processingTimerInterval) {
          clearInterval(processingTimerInterval);
          processingTimerInterval = null;
        }

        const totalTime = ((Date.now() - processingStartTime) / 1000).toFixed(
          1,
        );

        console.log(
          `[Preprocess] Complete! Processed ${framesProcessed} frames in ${totalTime}s`,
        );
        console.log(
          `[Preprocess] Frame annotations stored: ${frameAnnotations.size}`,
        );
        status.textContent = `âœ… Processing complete! ${framesProcessed} frames analyzed in ${totalTime}s. You can now watch the video.`;

        // Show video, canvas and enable controls
        video.style.display = "block";
        canvas.style.display = "block";

        // Ensure canvas matches video dimensions
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        canvas.style.width = video.clientWidth + "px";
        canvas.style.height = video.clientHeight + "px";

        video.currentTime = 0;

        // Set up playback
        video.removeEventListener("play", startProcessingOnPlay);
        video.addEventListener("play", startAnnotatedPlayback);
        video.addEventListener("ended", stopAnnotatedPlayback);
        video.addEventListener("pause", stopAnnotatedPlayback);
      }

      // Start playing with annotations
      function startAnnotatedPlayback() {
        if (!isProcessing && preprocessingComplete) {
          isProcessing = true;
          renderAnnotatedFrame();
        }
      }

      // Stop playback
      function stopAnnotatedPlayback() {
        isProcessing = false;
      }

      // Render frame with pre-computed annotations
      function renderAnnotatedFrame() {
        if (!isProcessing || video.paused || video.ended) {
          isProcessing = false;
          return;
        }

        // Find the frame closest to current time
        const currentFrameId = Math.floor(video.currentTime * videoFPS);

        // Draw the annotations for this frame
        if (frameAnnotations.has(currentFrameId)) {
          const annotation = frameAnnotations.get(currentFrameId);
          console.log(
            `[Playback] Drawing frame ${currentFrameId} at time ${video.currentTime.toFixed(2)}s`,
          );
          drawDetections(annotation);
        } else {
          console.log(
            `[Playback] No annotation for frame ${currentFrameId} at time ${video.currentTime.toFixed(2)}s`,
          );
          ctx.clearRect(0, 0, canvas.width, canvas.height);
        }

        requestAnimationFrame(renderAnnotatedFrame);
      }

      // Add job to queue (only used during preprocessing now)
      function enqueueJob(imageData) {
        const frameId = frameIdCounter++;

        const job = {
          frameId,
          timestamp: video.currentTime,
          imageData,
        };

        jobQueue.push(job);
        processNextJob();
      }

      // Placeholder for old play event (removed)
      function startProcessingOnPlay() {
        // This function is no longer used
      }

      // Preprocessing for main thread
      function preprocessOnMainThread(imageData) {
        const { data } = imageData;
        const float32Data = new Float32Array(
          3 * MODEL_INPUT_SIZE * MODEL_INPUT_SIZE,
        );

        for (let i = 0; i < data.length / 4; i++) {
          const r = data[i * 4] / 255.0;
          const g = data[i * 4 + 1] / 255.0;
          const b = data[i * 4 + 2] / 255.0;
          float32Data[i] = r;
          float32Data[i + MODEL_INPUT_SIZE * MODEL_INPUT_SIZE] = g;
          float32Data[i + 2 * MODEL_INPUT_SIZE * MODEL_INPUT_SIZE] = b;
        }

        return new ort.Tensor("float32", float32Data, [
          1,
          3,
          MODEL_INPUT_SIZE,
          MODEL_INPUT_SIZE,
        ]);
      }

      // --- 5. PRE-PROCESSING (Moved to Worker) ---
      // Preprocessing now happens in detection-worker.js for parallel processing

      // --- 6. POST-PROCESSING (YOLOv11/v8 Logic) ---
      function drawDetections(output) {
        ctx.clearRect(0, 0, canvas.width, canvas.height);

        const data = output.data;
        const dims = output.dims; // [1, 84, 8400]
        const numClasses = dims[1] - 4;
        const numAnchors = dims[2];

        const scaleX = canvas.width / MODEL_INPUT_SIZE;
        const scaleY = canvas.height / MODEL_INPUT_SIZE;

        const boxes = [];

        console.log(
          `[Draw] Canvas: ${canvas.width}x${canvas.height}, Scale: ${scaleX.toFixed(2)}x${scaleY.toFixed(2)}, Anchors: ${numAnchors}`,
        );

        for (let i = 0; i < numAnchors; i++) {
          let maxScore = -Infinity;
          let classIndex = -1;

          for (let c = 0; c < numClasses; c++) {
            const row = c + 4;
            const score = data[row * numAnchors + i];
            if (score > maxScore) {
              maxScore = score;
              classIndex = c;
            }
          }

          if (maxScore > CONFIDENCE_THRESHOLD) {
            // FILTER: Only allow Person (0) and Sports Ball (32)
            // if (classIndex === 0 || classIndex === 32) {
            const cx = data[0 * numAnchors + i];
            const cy = data[1 * numAnchors + i];
            const w = data[2 * numAnchors + i];
            const h = data[3 * numAnchors + i];

            const x1 = (cx - w / 2) * scaleX;
            const y1 = (cy - h / 2) * scaleY;
            const x2 = (cx + w / 2) * scaleX;
            const y2 = (cy + h / 2) * scaleY;

            boxes.push([x1, y1, x2, y2, classIndex, maxScore]);
            // }
          }
        }

        const selectedBoxes = nms(boxes, 0.45);

        console.log(
          `[Draw] Found ${boxes.length} boxes, after NMS: ${selectedBoxes.length}`,
        );

        selectedBoxes.forEach((box) => {
          const [x1, y1, x2, y2, classIdx, score] = box;
          const color = getColor(classIdx);
          const labelName = labels[classIdx] || "Unknown";

          ctx.strokeStyle = color;
          ctx.lineWidth = 3;
          ctx.strokeRect(x1, y1, x2 - x1, y2 - y1);

          const text = `${labelName} ${(score * 100).toFixed(1)}%`;
          ctx.font = "16px Arial";
          const textWidth = ctx.measureText(text).width;
          ctx.fillStyle = color;
          ctx.fillRect(x1, y1 - 20, textWidth + 10, 20);
          ctx.fillStyle = "#FFFFFF";
          ctx.fillText(text, x1 + 5, y1 - 5);
        });
      }

      function nms(boxes, iouThreshold) {
        if (boxes.length === 0) return [];
        boxes.sort((a, b) => b[5] - a[5]);
        const result = [];
        while (boxes.length > 0) {
          const best = boxes.shift();
          result.push(best);
          boxes = boxes.filter(
            (other) => calculateIoU(best, other) < iouThreshold,
          );
        }
        return result;
      }

      function calculateIoU(boxA, boxB) {
        const xA = Math.max(boxA[0], boxB[0]);
        const yA = Math.max(boxA[1], boxB[1]);
        const xB = Math.min(boxA[2], boxB[2]);
        const yB = Math.min(boxA[3], boxB[3]);
        const intersectionArea = Math.max(0, xB - xA) * Math.max(0, yB - yA);
        const areaA = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1]);
        const areaB = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1]);
        return intersectionArea / (areaA + areaB - intersectionArea);
      }

      function getColor(index) {
        const colors = [
          "#FF0000",
          "#00FF00",
          "#0000FF",
          "#000000",
          "#00FFFF",
          "#FF00FF",
          "#FFA500",
        ];
        return colors[index % colors.length];
      }
    </script>
  </body>
</html>
